import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import os
import sys
import re

# UTF-8 ì¶œë ¥ ì„¤ì • (Windows)
if sys.platform == 'win32':
    os.system('chcp 65001 > nul')

def detect_category(keyword):
    """í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ë¥¼ ìë™ ì¸ì‹í•©ë‹ˆë‹¤."""

    # ê±°ì‹œê²½ì œ í‚¤ì›Œë“œ
    macro_keywords = [
        'ì¦ì‹œ', 'í™˜ìœ¨', 'ê¸ˆë¦¬', 'ìœ ê°€', 'ë‹¬ëŸ¬', 'ì›í™”', 'ì¸í”Œë ˆì´ì…˜',
        'ì—°ì¤€', 'Fed', 'ê¸°ì¤€ê¸ˆë¦¬', 'í†µí™”ì •ì±…', 'ê²½ì œ', 'GDP', 'CPI',
        'ë¬´ì—­', 'ê´€ì„¸', 'ê²½ê¸°', 'ì¹¨ì²´', 'ê²½ì œì„±ì¥', 'ì¬ì •', 'ì˜ˆì‚°'
    ]

    # ë¯¸êµ­ ì‹œì¥ í‚¤ì›Œë“œ
    us_market_keywords = [
        'ë¯¸êµ­', 'US', 'ë‚˜ìŠ¤ë‹¥', 'ë‹¤ìš°', 'S&P', 'NASDAQ', 'NYSE',
        'ì›”ìŠ¤íŠ¸ë¦¬íŠ¸', 'ì‹¤ë¦¬ì½˜ë°¸ë¦¬'
    ]

    # ì‚°ì—…/í…Œë§ˆ í‚¤ì›Œë“œ
    theme_keywords = [
        'AI', 'ë°˜ë„ì²´', '2ì°¨ì „ì§€', 'ë°”ì´ì˜¤', 'ì›ìë ¥', 'ìˆ˜ì†Œ', 'ESS',
        'ì „ê¸°ì°¨', 'EV', 'ë¡œë´‡', 'ìš°ì£¼', 'ë°©ì‚°', 'ì œì•½', 'IT',
        'í´ë¼ìš°ë“œ', 'ë©”íƒ€ë²„ìŠ¤', 'NFT', 'ë¸”ë¡ì²´ì¸'
    ]

    # ë¯¸êµ­ ê¸°ì—… í‚¤ì›Œë“œ
    us_companies = [
        'ì—”ë¹„ë””ì•„', 'NVIDIA', 'í…ŒìŠ¬ë¼', 'Tesla', 'ì• í”Œ', 'Apple',
        'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'Microsoft', 'ì•„ë§ˆì¡´', 'Amazon', 'êµ¬ê¸€', 'Google',
        'ë©”íƒ€', 'Meta', 'AMD', 'ì¸í…”', 'Intel'
    ]

    keyword_lower = keyword.lower()

    # ì¹´í…Œê³ ë¦¬ íŒì •
    if any(k in keyword for k in us_market_keywords):
        return 'ë¯¸êµ­ì¦ì‹œ'
    elif any(k in keyword for k in macro_keywords):
        return 'ê±°ì‹œê²½ì œ'
    elif any(k in keyword for k in us_companies):
        return 'ë¯¸êµ­ê¸°ì—…'
    elif any(k in keyword for k in theme_keywords):
        return 'ì‚°ì—…í…Œë§ˆ'
    else:
        return 'êµ­ë‚´ì¢…ëª©'

def get_enhanced_keywords(category):
    """ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""

    # ê¸°ë³¸ í˜¸ì¬/ì•…ì¬ í‚¤ì›Œë“œ
    base_positive = [
        'ìƒìŠ¹', 'í˜¸ì¬', 'ì¦ê°€', 'ì„±ì¥', 'í™•ëŒ€', 'ê°œì„ ', 'í‘ì', 'ìˆ˜ì£¼', 'ê³„ì•½',
        'ì‹ ê·œ', 'ì¶œì‹œ', 'ê°œë°œ', 'ì„±ê³µ', 'í˜¸ì¡°', 'ìƒí–¥', 'ê¸ì •', 'íˆ¬ì', 'í˜‘ë ¥',
        'ì œíœ´', 'íŠ¹í—ˆ', 'ìˆ˜ì¶œ', 'ì‹¤ì ', 'ë°°ë‹¹', 'ì¦ì„¤', 'í™•ì¥', 'ì§„ì¶œ', 'ë„ì•½',
        'ìˆ˜ìµ', 'ë§¤ì¶œ', 'ìˆœì´ìµ', 'ëŒíŒŒ', 'ì‹ ê¸°ë¡', 'ìµœê³ ', 'ê¸‰ë“±', 'ê°•ì„¸'
    ]

    base_negative = [
        'í•˜ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ë¶€ì§„', 'ì ì', 'ì·¨ì†Œ', 'ì§€ì—°', 'ì‹¤íŒ¨',
        'í•˜í–¥', 'ë¶€ì •', 'ë¦¬ìŠ¤í¬', 'ìš°ë ¤', 'ì†ì‹¤', 'ê°ì‚¬', 'ê·œì œ', 'ì†Œì†¡',
        'ì‚¬ê³ ', 'ë¬¸ì œ', 'ìœ„ë°˜', 'ì¤‘ë‹¨', 'íê¸°', 'ì² ìˆ˜', 'íŒŒì‚°', 'êµ¬ì¡°ì¡°ì •',
        'ì ì', 'ì†í•´', 'ê¸‰ë½', 'ì•½ì„¸', 'ìœ„ê¸°', 'ë¶ˆì•ˆ', 'ì¹¨ì²´'
    ]

    # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ê°€ í‚¤ì›Œë“œ
    if category == 'ë¯¸êµ­ì¦ì‹œ':
        positive_add = [
            'ë ë¦¬', 'ì‚¬ìƒìµœê³ ', 'ATH', 'ì‚¬ê³ ê°€', 'ê¸°ë¡ê²½ì‹ ', 'ë¶ˆê¸°ë‘¥',
            'í˜¸í™©', 'ë°˜ë“±', 'ìƒìŠ¹ì¥', 'ê°•ì„¸ì¥', 'ì‹ ê³ ê°€', 'ê¸‰ë“±',
            'ë§¤ìˆ˜ì„¸', 'ìê¸ˆìœ ì…', 'ìˆ˜ê¸‰', 'ì¸ìƒ', 'í€ë”ë©˜í„¸', 'ë°¸ë¥˜ì—ì´ì…˜'
        ]
        negative_add = [
            'í­ë½', 'ê¸‰ë½', 'ì•½ì„¸ì¥', 'í•˜ë½ì¥', 'ì¡°ì •', 'íŒ¨ë‹‰',
            'ì°¨ìµì‹¤í˜„', 'ë§¤ë„ì„¸', 'ìê¸ˆì´íƒˆ', 'ë²„ë¸”', 'ê³µí¬', 'ë³€ë™ì„±',
            'í•˜ë½ì„¸', 'ì¹¨ì²´', 'ìš°ë ¤', 'ê¸´ì¶•', 'ê¸ˆë¦¬ì¸ìƒ'
        ]

    elif category == 'ê±°ì‹œê²½ì œ':
        positive_add = [
            'ê¸ˆë¦¬ì¸í•˜', 'ì™„í™”', 'ë¶€ì–‘', 'ê²½ê¸°íšŒë³µ', 'í˜¸ì „', 'ë°˜ë“±',
            'í™•ì¥', 'ê°œì„ ', 'ì„±ì¥ë¥ ', 'ê³ ìš©ì¦ê°€', 'ì†Œë¹„ì¦ê°€', 'ìˆ˜ì¶œì¦ê°€',
            'íˆ¬ìì¦ê°€', 'ê²½ê¸°í™•ì¥', 'ì¸í•˜', 'ê²½ê¸°íšŒë³µ', 'ì•ˆì •'
        ]
        negative_add = [
            'ê¸ˆë¦¬ì¸ìƒ', 'ê¸´ì¶•', 'ê²½ê¸°ì¹¨ì²´', 'ë¶ˆí™©', 'ìœ„ì¶•', 'ë‘”í™”',
            'í•˜ë½', 'ê°ì†Œ', 'ë¦¬ì„¸ì…˜', 'ê²½ê¸°í›„í‡´', 'ê²½ê¸°ìœ„ì¶•', 'ì¸ìƒ',
            'ìœ„ê¸°', 'ë¶•ê´´', 'ë¶ˆì•ˆ', 'ê¸‰ë“±', 'ë¬¼ê°€ìƒìŠ¹'
        ]

    elif category in ['ë¯¸êµ­ê¸°ì—…', 'ì‚°ì—…í…Œë§ˆ', 'êµ­ë‚´ì¢…ëª©']:
        positive_add = [
            'FDAìŠ¹ì¸', 'ê³„ì•½ì²´ê²°', 'ìˆ˜ì£¼', 'ë§¤ì¶œì¦ê°€', 'ì‹ ì œí’ˆ', 'í˜ì‹ ',
            'ê¸°ìˆ ê°œë°œ', 'íŠ¹í—ˆë“±ë¡', 'ì¸ìˆ˜í•©ë³‘', 'M&A', 'íˆ¬ììœ ì¹˜', 'ìƒì¥',
            'IPO', 'ì‹œì¥ì ìœ ìœ¨', 'ê²½ìŸë ¥', 'ê¸€ë¡œë²Œì§„ì¶œ', 'íŒŒíŠ¸ë„ˆì‹­'
        ]
        negative_add = [
            'ë¦¬ì½œ', 'ì œì¬', 'ë²Œê¸ˆ', 'ì˜ì—…ì •ì§€', 'í—ˆê°€ì·¨ì†Œ', 'ì„ìƒì‹¤íŒ¨',
            'ê³„ì•½ì·¨ì†Œ', 'ìˆ˜ì£¼ì·¨ì†Œ', 'ì†ì‹¤', 'ë§¤ì¶œê°ì†Œ', 'ì‹œì¥ì´íƒˆ',
            'ê²½ìŸì‹¬í™”', 'ì ìœ ìœ¨í•˜ë½', 'ì‹¤ì ë¶€ì§„', 'ì ìì§€ì†'
        ]
    else:
        positive_add = []
        negative_add = []

    return {
        'positive': base_positive + positive_add,
        'negative': base_negative + negative_add
    }

def get_report_path(keyword, date_str=None):
    """ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')

    # news í´ë” ìƒì„±
    report_dir = os.path.join('report', date_str, 'news')
    os.makedirs(report_dir, exist_ok=True)

    # íŒŒì¼ëª…ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
    safe_keyword = re.sub(r'[\\/*?:"<>|]', '', keyword)
    return os.path.join(report_dir, f'news_{safe_keyword}.md')

def crawl_naver_news(keyword, days=14):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    print(f"\n'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    print(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days}ì¼")

    # ë‚ ì§œ ê³„ì‚°
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)

    # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
    base_url = "https://search.naver.com/search.naver"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    articles = []

    # ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘ (ìµœëŒ€ 5í˜ì´ì§€ë¡œ ì¦ê°€)
    for page in range(1, 6):
        params = {
            'where': 'news',
            'query': keyword,
            'start': (page - 1) * 10 + 1,
            'sort': '1'  # ìµœì‹ ìˆœ ì •ë ¬
        }

        try:
            response = requests.get(base_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰
            news_items = soup.select('.vs1RfKE1eTzMZ5RqnhIv')

            if not news_items:
                print(f"  í˜ì´ì§€ {page}: ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            for item in news_items:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.select_one('a[data-heatmap-target=".tit"] span')
                    if not title_elem:
                        continue

                    # <mark> íƒœê·¸ ì œê±°
                    for mark in title_elem.find_all('mark'):
                        mark.unwrap()

                    title = title_elem.get_text(strip=True)

                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.select_one('a[data-heatmap-target=".tit"]')
                    link = link_elem.get('href', '') if link_elem else ''

                    # ë³¸ë¬¸ ìš”ì•½ ì¶”ì¶œ
                    content_elem = item.select_one('a[data-heatmap-target=".body"] span')
                    if content_elem:
                        for mark in content_elem.find_all('mark'):
                            mark.unwrap()
                        content = content_elem.get_text(strip=True)
                    else:
                        content = ''

                    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                    source_elem = item.select_one('a[data-heatmap-target=".prof"] span')
                    source = source_elem.get_text(strip=True) if source_elem else 'ì¶œì²˜ ë¯¸ìƒ'

                    # ë‚ ì§œ ì¶”ì¶œ
                    date_elem = item.select_one('.U1zN1wdZWj0pyvj9oyR0 span')
                    date_text = date_elem.get_text(strip=True) if date_elem else ''

                    # ë‚ ì§œ íŒŒì‹±
                    article_date = parse_date(date_text)

                    # ê¸°ê°„ í•„í„°ë§
                    if article_date and article_date < start_date:
                        continue

                    articles.append({
                        'title': title,
                        'content': content,
                        'source': source,
                        'date': article_date,
                        'date_text': date_text,
                        'link': link
                    })

                except Exception as e:
                    print(f"  ê¸°ì‚¬ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            print(f"  í˜ì´ì§€ {page}: {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")

            # ìš”ì²­ ê°„ê²© (ë„¤ì´ë²„ ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)

        except requests.RequestException as e:
            print(f"  í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    articles.sort(key=lambda x: x['date'] if x['date'] else datetime.min, reverse=True)

    print(f"\nì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")

    return articles

def parse_date(date_text):
    """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        now = datetime.now()

        # "në¶„ ì „", "nì‹œê°„ ì „" í˜•ì‹
        if 'ë¶„ ì „' in date_text or 'ë¶„ì „' in date_text:
            minutes = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(minutes=minutes)
        elif 'ì‹œê°„ ì „' in date_text or 'ì‹œê°„ì „' in date_text:
            hours = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(hours=hours)
        elif 'ì¼ ì „' in date_text or 'ì¼ì „' in date_text:
            days = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(days=days)

        # "YYYY.MM.DD" í˜•ì‹
        date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_text)
        if date_match:
            year, month, day = date_match.groups()
            return datetime(int(year), int(month), int(day))

        return None

    except:
        return None

def analyze_sentiment(articles, category):
    """ê¸°ì‚¬ì˜ í˜¸ì¬/ì•…ì¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ í‚¤ì›Œë“œ ê¸°ë°˜)"""

    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    keywords = get_enhanced_keywords(category)
    positive_keywords = keywords['positive']
    negative_keywords = keywords['negative']

    analyzed = []

    for article in articles:
        text = article['title'] + ' ' + article['content']

        # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
        positive_count = sum(1 for keyword in positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text)

        # ê°ì„± ë¶„ë¥˜
        if positive_count > negative_count:
            sentiment = 'í˜¸ì¬'
            score = min(positive_count, 5)  # ìµœëŒ€ 5ì 
        elif negative_count > positive_count:
            sentiment = 'ì•…ì¬'
            score = min(negative_count, 5)
        else:
            sentiment = 'ì¤‘ë¦½'
            score = 0

        # ì¤‘ìš”ë„ ê³„ì‚° (í˜¸ì¬/ì•…ì¬ í‚¤ì›Œë“œ ê°œìˆ˜ + ìµœì‹ ì„±)
        importance = positive_count + negative_count

        analyzed.append({
            **article,
            'sentiment': sentiment,
            'score': score,
            'importance': importance,
            'positive_count': positive_count,
            'negative_count': negative_count
        })

    return analyzed

def generate_enhanced_report(keyword, analyzed_articles, category, date_str=None):
    """ê°œì„ ëœ ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""

    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')

    # í˜¸ì¬/ì•…ì¬/ì¤‘ë¦½ ë¶„ë¥˜ ë° ì¤‘ìš”ë„ìˆœ ì •ë ¬
    positive_news = sorted([a for a in analyzed_articles if a['sentiment'] == 'í˜¸ì¬'],
                          key=lambda x: x['importance'], reverse=True)
    negative_news = sorted([a for a in analyzed_articles if a['sentiment'] == 'ì•…ì¬'],
                          key=lambda x: x['importance'], reverse=True)
    neutral_news = [a for a in analyzed_articles if a['sentiment'] == 'ì¤‘ë¦½']

    # ì¹´í…Œê³ ë¦¬ë³„ ì•„ì´ì½˜
    category_icons = {
        'ë¯¸êµ­ì¦ì‹œ': 'ğŸ‡ºğŸ‡¸',
        'ê±°ì‹œê²½ì œ': 'ğŸ“Š',
        'ë¯¸êµ­ê¸°ì—…': 'ğŸ¢',
        'ì‚°ì—…í…Œë§ˆ': 'ğŸ­',
        'êµ­ë‚´ì¢…ëª©': 'ğŸ“ˆ'
    }

    icon = category_icons.get(category, 'ğŸ“°')

    md_content = f"""# {icon} {keyword} ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸

**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ**: {date_str}
**ìˆ˜ì§‘ ê¸°ê°„**: ìµœê·¼ 14ì¼
**ì¹´í…Œê³ ë¦¬**: {category}

---

## ğŸ“Š ìš”ì•½ í†µê³„

| êµ¬ë¶„ | ê±´ìˆ˜ | ë¹„ìœ¨ |
|------|------|------|
| **ì „ì²´ ê¸°ì‚¬** | {len(analyzed_articles)}ê±´ | 100% |
| ğŸŸ¢ **í˜¸ì¬ì„± ê¸°ì‚¬** | {len(positive_news)}ê±´ | {len(positive_news)/len(analyzed_articles)*100:.1f}% |
| ğŸ”´ **ì•…ì¬ì„± ê¸°ì‚¬** | {len(negative_news)}ê±´ | {len(negative_news)/len(analyzed_articles)*100:.1f}% |
| âšª **ì¤‘ë¦½ ê¸°ì‚¬** | {len(neutral_news)}ê±´ | {len(neutral_news)/len(analyzed_articles)*100:.1f}% |

### ğŸ’¹ ì‹œì¥ ë¶„ìœ„ê¸°
"""

    # ì‹œì¥ ë¶„ìœ„ê¸° íŒë‹¨
    sentiment_ratio = len(positive_news) / len(analyzed_articles) if analyzed_articles else 0

    if sentiment_ratio >= 0.7:
        market_mood = "**ë§¤ìš° ê¸ì •ì ** ğŸ”¥ - ê°•í•œ í˜¸ì¬ ë‰´ìŠ¤ ìš°ì„¸"
    elif sentiment_ratio >= 0.5:
        market_mood = "**ê¸ì •ì ** âœ… - í˜¸ì¬ ë‰´ìŠ¤ ìš°ì„¸"
    elif sentiment_ratio >= 0.4:
        market_mood = "**ì¤‘ë¦½ì ** âš–ï¸ - í˜¸ì¬ì™€ ì•…ì¬ ê· í˜•"
    elif sentiment_ratio >= 0.2:
        market_mood = "**ë¶€ì •ì ** âš ï¸ - ì•…ì¬ ë‰´ìŠ¤ ìš°ì„¸"
    else:
        market_mood = "**ë§¤ìš° ë¶€ì •ì ** ğŸš¨ - ê°•í•œ ì•…ì¬ ë‰´ìŠ¤ ìš°ì„¸"

    md_content += f"- {market_mood}\n"
    md_content += f"- í˜¸ì¬/ì•…ì¬ ë¹„ìœ¨: {len(positive_news)}:{len(negative_news)}\n\n"

    md_content += "---\n\n"

    # í˜¸ì¬ì„± ë‰´ìŠ¤
    md_content += f"## ğŸŸ¢ í˜¸ì¬ì„± ë‰´ìŠ¤ ({len(positive_news)}ê±´)\n\n"

    if positive_news:
        for idx, article in enumerate(positive_news, 1):
            md_content += f"""### {idx}. {article['title']}

- **ì–¸ë¡ ì‚¬**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}
- **ì¤‘ìš”ë„**: {'â­' * article['score']} ({article['positive_count']}ê°œ í˜¸ì¬ í‚¤ì›Œë“œ)

**ğŸ“ ìš”ì•½**:
{article['content']}

[ğŸ“° ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "í˜¸ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    # ì•…ì¬ì„± ë‰´ìŠ¤
    md_content += f"## ğŸ”´ ì•…ì¬ì„± ë‰´ìŠ¤ ({len(negative_news)}ê±´)\n\n"

    if negative_news:
        for idx, article in enumerate(negative_news, 1):
            md_content += f"""### {idx}. {article['title']}

- **ì–¸ë¡ ì‚¬**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}
- **ì¤‘ìš”ë„**: {'â­' * article['score']} ({article['negative_count']}ê°œ ì•…ì¬ í‚¤ì›Œë“œ)

**ğŸ“ ìš”ì•½**:
{article['content']}

[ğŸ“° ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "ì•…ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    # ì¤‘ë¦½ ë‰´ìŠ¤ (ìµœëŒ€ 10ê°œ)
    md_content += f"## âšª ì¤‘ë¦½ ë‰´ìŠ¤ ({len(neutral_news)}ê±´)\n\n"

    if neutral_news:
        display_count = min(len(neutral_news), 10)
        md_content += f"*ìµœì‹  {display_count}ê±´ë§Œ í‘œì‹œ*\n\n"

        for idx, article in enumerate(neutral_news[:display_count], 1):
            md_content += f"""### {idx}. {article['title']}

- **ì–¸ë¡ ì‚¬**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}

**ğŸ“ ìš”ì•½**:
{article['content']}

[ğŸ“° ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "ì¤‘ë¦½ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    # ë¶„ì„ ì¸ì‚¬ì´íŠ¸
    md_content += """## ğŸ’¡ ë¶„ì„ ì¸ì‚¬ì´íŠ¸

### ğŸ“Œ ì£¼ìš” í¬ì¸íŠ¸
"""

    # Top 3 í˜¸ì¬/ì•…ì¬ ì¶”ì¶œ
    if positive_news:
        md_content += "\n#### ğŸŸ¢ ì£¼ìš” í˜¸ì¬\n"
        for i, article in enumerate(positive_news[:3], 1):
            md_content += f"{i}. {article['title']}\n"

    if negative_news:
        md_content += "\n#### ğŸ”´ ì£¼ìš” ì•…ì¬\n"
        for i, article in enumerate(negative_news[:3], 1):
            md_content += f"{i}. {article['title']}\n"

    md_content += """

### âš ï¸ íˆ¬ì ì‹œ ìœ ì˜ì‚¬í•­
- ë³¸ ë¶„ì„ì€ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ë¶„ì„ìœ¼ë¡œ **ì°¸ê³ ìš©**ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì‹¤ì œ íˆ¬ì ê²°ì •ì€ **ì „ë¬¸ê°€ì˜ ì¡°ì–¸**ê³¼ **ì¢…í•©ì ì¸ ë¶„ì„**ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ë‰´ìŠ¤ì˜ ì¤‘ìš”ë„ëŠ” í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¨ìˆœ ì§€í‘œì…ë‹ˆë‹¤.
- ìµœì‹  ë‰´ìŠ¤ì¼ìˆ˜ë¡ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“Œ ì°¸ê³ ì‚¬í•­

- **ë¶„ì„ ë°©ë²•**: ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ + í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„
- **ë°ì´í„° ì¶œì²˜**: ë„¤ì´ë²„ ë‰´ìŠ¤ í†µí•©ê²€ìƒ‰
- **ë¶„ì„ í‚¤ì›Œë“œ ìˆ˜**: í˜¸ì¬ í‚¤ì›Œë“œ ì•½ 50ê°œ, ì•…ì¬ í‚¤ì›Œë“œ ì•½ 50ê°œ
- **ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ ë¶„ì„**: ë¯¸êµ­ì¦ì‹œ, ê±°ì‹œê²½ì œ, ì‚°ì—…í…Œë§ˆ ë“± ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì ìš©

---

*ğŸ¤– Generated with Claude Code (Enhanced Version)*
*ğŸ“… Generated at: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + "*"

    output_file = get_report_path(keyword, date_str)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"\n[OK] ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return output_file

def safe_print(text):
    """ì´ëª¨ì§€ë¥¼ ì•ˆì „í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    try:
        print(text)
    except UnicodeEncodeError:
        # ì´ëª¨ì§€ ì œê±° í›„ ì¶œë ¥
        text_clean = text.encode('ascii', 'ignore').decode('ascii')
        print(text_clean)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    safe_print("ğŸ“° ê°œì„ ëœ ë‰´ìŠ¤ ë¶„ì„ ë„êµ¬ (Enhanced Version)")
    print("=" * 80)

    # í‚¤ì›Œë“œ ì…ë ¥
    if len(sys.argv) > 1:
        keyword = sys.argv[1]
    else:
        keyword = input("\në¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ëª©ëª…/í…Œë§ˆ/ì‹œì¥): ").strip()

    if not keyword:
        print("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    try:
        # ì¹´í…Œê³ ë¦¬ ìë™ ì¸ì‹
        category = detect_category(keyword)
        safe_print(f"\nğŸ·ï¸  ì¸ì‹ëœ ì¹´í…Œê³ ë¦¬: {category}")

        # 1. ë‰´ìŠ¤ í¬ë¡¤ë§
        articles = crawl_naver_news(keyword, days=14)

        if not articles:
            print("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. ê°ì„± ë¶„ì„ (ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤)
        safe_print(f"\nğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì¤‘... (ì¹´í…Œê³ ë¦¬: {category})")
        analyzed = analyze_sentiment(articles, category)

        # 3. ê°œì„ ëœ ë¦¬í¬íŠ¸ ìƒì„±
        output_file = generate_enhanced_report(keyword, analyzed, category)

        # 4. ìš”ì•½ ì¶œë ¥
        positive_count = sum(1 for a in analyzed if a['sentiment'] == 'í˜¸ì¬')
        negative_count = sum(1 for a in analyzed if a['sentiment'] == 'ì•…ì¬')
        neutral_count = sum(1 for a in analyzed if a['sentiment'] == 'ì¤‘ë¦½')

        print("\n" + "=" * 80)
        safe_print("âœ… ë¶„ì„ ì™„ë£Œ!")
        print("=" * 80)
        safe_print(f"ğŸ“° ì „ì²´ ê¸°ì‚¬: {len(analyzed)}ê±´")
        safe_print(f"ğŸŸ¢ í˜¸ì¬ì„± ê¸°ì‚¬: {positive_count}ê±´ ({positive_count/len(analyzed)*100:.1f}%)")
        safe_print(f"ğŸ”´ ì•…ì¬ì„± ê¸°ì‚¬: {negative_count}ê±´ ({negative_count/len(analyzed)*100:.1f}%)")
        safe_print(f"âšª ì¤‘ë¦½ ê¸°ì‚¬: {neutral_count}ê±´ ({neutral_count/len(analyzed)*100:.1f}%)")
        safe_print(f"\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬: {category}")
        safe_print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
        print("=" * 80)

    except Exception as e:
        safe_print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
