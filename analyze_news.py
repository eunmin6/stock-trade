import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import os
import sys
import re

def get_report_path(stock_name, date_str=None):
    """ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')

    # news í´ë” ìƒì„±
    report_dir = os.path.join('report', date_str, 'news')
    os.makedirs(report_dir, exist_ok=True)

    # íŒŒì¼ëª…ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
    safe_stock_name = re.sub(r'[\\/*?:"<>|]', '', stock_name)
    return os.path.join(report_dir, f'news_{safe_stock_name}.md')

def crawl_naver_news(stock_name, days=14):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    print(f"\n'{stock_name}' ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    print(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days}ì¼")

    # ë‚ ì§œ ê³„ì‚°
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)

    # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
    base_url = "https://search.naver.com/search.naver"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    articles = []

    # ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘ (ìµœëŒ€ 3í˜ì´ì§€)
    for page in range(1, 4):
        params = {
            'where': 'news',
            'query': stock_name,
            'start': (page - 1) * 10 + 1,
            'sort': '1'  # ìµœì‹ ìˆœ ì •ë ¬
        }

        try:
            response = requests.get(base_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ (ìƒˆë¡œìš´ HTML êµ¬ì¡°)
            news_items = soup.select('.vs1RfKE1eTzMZ5RqnhIv')

            if not news_items:
                print(f"  í˜ì´ì§€ {page}: ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            for item in news_items:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.select_one('a[data-heatmap-target=".tit"] span')
                    if not title_elem:
                        continue

                    # <mark> íƒœê·¸ ì œê±°
                    for mark in title_elem.find_all('mark'):
                        mark.unwrap()

                    title = title_elem.get_text(strip=True)

                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.select_one('a[data-heatmap-target=".tit"]')
                    link = link_elem.get('href', '') if link_elem else ''

                    # ë³¸ë¬¸ ìš”ì•½ ì¶”ì¶œ
                    content_elem = item.select_one('a[data-heatmap-target=".body"] span')
                    if content_elem:
                        # <mark> íƒœê·¸ ì œê±°
                        for mark in content_elem.find_all('mark'):
                            mark.unwrap()
                        content = content_elem.get_text(strip=True)
                    else:
                        content = ''

                    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                    source_elem = item.select_one('a[data-heatmap-target=".prof"] span')
                    source = source_elem.get_text(strip=True) if source_elem else 'ì¶œì²˜ ë¯¸ìƒ'

                    # ë‚ ì§œ ì¶”ì¶œ
                    date_elem = item.select_one('.U1zN1wdZWj0pyvj9oyR0 span')
                    date_text = date_elem.get_text(strip=True) if date_elem else ''

                    # ë‚ ì§œ íŒŒì‹±
                    article_date = parse_date(date_text)

                    # ê¸°ê°„ í•„í„°ë§
                    if article_date and article_date < start_date:
                        continue

                    articles.append({
                        'title': title,
                        'content': content,
                        'source': source,
                        'date': article_date,
                        'date_text': date_text,
                        'link': link
                    })

                except Exception as e:
                    print(f"  ê¸°ì‚¬ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            print(f"  í˜ì´ì§€ {page}: {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")

            # ìš”ì²­ ê°„ê²© (ë„¤ì´ë²„ ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)

        except requests.RequestException as e:
            print(f"  í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    articles.sort(key=lambda x: x['date'] if x['date'] else datetime.min, reverse=True)

    print(f"\nì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")

    return articles

def parse_date(date_text):
    """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        now = datetime.now()

        # "në¶„ ì „", "nì‹œê°„ ì „" í˜•ì‹
        if 'ë¶„ ì „' in date_text or 'ë¶„ì „' in date_text:
            minutes = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(minutes=minutes)
        elif 'ì‹œê°„ ì „' in date_text or 'ì‹œê°„ì „' in date_text:
            hours = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(hours=hours)
        elif 'ì¼ ì „' in date_text or 'ì¼ì „' in date_text:
            days = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(days=days)

        # "YYYY.MM.DD" í˜•ì‹
        date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_text)
        if date_match:
            year, month, day = date_match.groups()
            return datetime(int(year), int(month), int(day))

        return None

    except:
        return None

def analyze_sentiment(articles):
    """ê¸°ì‚¬ì˜ í˜¸ì¬/ì•…ì¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ì„)"""

    # í˜¸ì¬ í‚¤ì›Œë“œ
    positive_keywords = [
        'ìƒìŠ¹', 'í˜¸ì¬', 'ì¦ê°€', 'ì„±ì¥', 'í™•ëŒ€', 'ê°œì„ ', 'í‘ì', 'ìˆ˜ì£¼', 'ê³„ì•½',
        'ì‹ ê·œ', 'ì¶œì‹œ', 'ê°œë°œ', 'ì„±ê³µ', 'í˜¸ì¡°', 'ìƒí–¥', 'ê¸ì •', 'íˆ¬ì', 'í˜‘ë ¥',
        'ì œíœ´', 'íŠ¹í—ˆ', 'ìˆ˜ì¶œ', 'ì‹¤ì ', 'ë°°ë‹¹', 'ì¦ì„¤', 'í™•ì¥', 'ì§„ì¶œ'
    ]

    # ì•…ì¬ í‚¤ì›Œë“œ
    negative_keywords = [
        'í•˜ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ë¶€ì§„', 'ì ì', 'ì·¨ì†Œ', 'ì§€ì—°', 'ì‹¤íŒ¨',
        'í•˜í–¥', 'ë¶€ì •', 'ë¦¬ìŠ¤í¬', 'ìš°ë ¤', 'ì†ì‹¤', 'ì ì', 'ê°ì‚¬', 'ê·œì œ', 'ì†Œì†¡',
        'ì‚¬ê³ ', 'ë¬¸ì œ', 'ìœ„ë°˜', 'ì¤‘ë‹¨', 'íê¸°', 'ì² ìˆ˜', 'íŒŒì‚°', 'êµ¬ì¡°ì¡°ì •'
    ]

    analyzed = []

    for article in articles:
        text = article['title'] + ' ' + article['content']

        # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
        positive_count = sum(1 for keyword in positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text)

        # ê°ì„± ë¶„ë¥˜
        if positive_count > negative_count:
            sentiment = 'í˜¸ì¬'
            score = min(positive_count, 5)  # ìµœëŒ€ 5ì 
        elif negative_count > positive_count:
            sentiment = 'ì•…ì¬'
            score = min(negative_count, 5)
        else:
            sentiment = 'ì¤‘ë¦½'
            score = 0

        analyzed.append({
            **article,
            'sentiment': sentiment,
            'score': score,
            'positive_count': positive_count,
            'negative_count': negative_count
        })

    return analyzed

def generate_news_report(stock_name, analyzed_articles, date_str=None):
    """ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""

    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')

    # í˜¸ì¬/ì•…ì¬/ì¤‘ë¦½ ë¶„ë¥˜
    positive_news = [a for a in analyzed_articles if a['sentiment'] == 'í˜¸ì¬']
    negative_news = [a for a in analyzed_articles if a['sentiment'] == 'ì•…ì¬']
    neutral_news = [a for a in analyzed_articles if a['sentiment'] == 'ì¤‘ë¦½']

    md_content = f"""# {stock_name} ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸

**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ**: {date_str}
**ìˆ˜ì§‘ ê¸°ê°„**: ìµœê·¼ 14ì¼

---

## ğŸ“Š ìš”ì•½

| êµ¬ë¶„ | ê±´ìˆ˜ |
|------|------|
| ì „ì²´ ê¸°ì‚¬ | {len(analyzed_articles)}ê±´ |
| í˜¸ì¬ì„± ê¸°ì‚¬ | {len(positive_news)}ê±´ |
| ì•…ì¬ì„± ê¸°ì‚¬ | {len(negative_news)}ê±´ |
| ì¤‘ë¦½ ê¸°ì‚¬ | {len(neutral_news)}ê±´ |

---

## ğŸŸ¢ í˜¸ì¬ì„± ë‰´ìŠ¤ ({len(positive_news)}ê±´)

"""

    if positive_news:
        for idx, article in enumerate(positive_news, 1):
            md_content += f"""### {idx}. {article['title']}

- **ì–¸ë¡ ì‚¬**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}
- **ì˜í–¥ë„**: {'â­' * article['score']}

**ìš”ì•½**:
{article['content']}

[ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "í˜¸ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    md_content += f"""## ğŸ”´ ì•…ì¬ì„± ë‰´ìŠ¤ ({len(negative_news)}ê±´)

"""

    if negative_news:
        for idx, article in enumerate(negative_news, 1):
            md_content += f"""### {idx}. {article['title']}

- **ì–¸ë¡ ì‚¬**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}
- **ì˜í–¥ë„**: {'â­' * article['score']}

**ìš”ì•½**:
{article['content']}

[ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "ì•…ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    md_content += f"""## âšª ì¤‘ë¦½ ë‰´ìŠ¤ ({len(neutral_news)}ê±´)

"""

    if neutral_news:
        for idx, article in enumerate(neutral_news[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            md_content += f"""### {idx}. {article['title']}

- **ì–¸ë¡ ì‚¬**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}

**ìš”ì•½**:
{article['content']}

[ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "ì¤‘ë¦½ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    md_content += """
## ğŸ“Œ ì°¸ê³ ì‚¬í•­

- ì´ ë¶„ì„ì€ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ë¶„ì„ìœ¼ë¡œ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì‹¤ì œ íˆ¬ì ê²°ì •ì€ ì „ë¬¸ê°€ì˜ ì¡°ì–¸ê³¼ ì¢…í•©ì ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ë‰´ìŠ¤ì˜ ì˜í–¥ë„ëŠ” í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¨ìˆœ ì§€í‘œì…ë‹ˆë‹¤.

---

*ğŸ¤– Generated with Claude Code*
"""

    output_file = get_report_path(stock_name, date_str)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"\në‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return output_file

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ì£¼ì‹ ë‰´ìŠ¤ ë¶„ì„ ë„êµ¬")
    print("=" * 80)

    # ì¢…ëª©ëª… ì…ë ¥
    if len(sys.argv) > 1:
        stock_name = sys.argv[1]
    else:
        stock_name = input("\në¶„ì„í•  ì¢…ëª©ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if not stock_name:
        print("ì¢…ëª©ëª…ì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    try:
        # 1. ë‰´ìŠ¤ í¬ë¡¤ë§
        articles = crawl_naver_news(stock_name, days=14)

        if not articles:
            print("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. ê°ì„± ë¶„ì„
        print("ë‰´ìŠ¤ ë¶„ì„ ì¤‘...")
        analyzed = analyze_sentiment(articles)

        # 3. ë¦¬í¬íŠ¸ ìƒì„±
        output_file = generate_news_report(stock_name, analyzed)

        # 4. ìš”ì•½ ì¶œë ¥
        positive_count = sum(1 for a in analyzed if a['sentiment'] == 'í˜¸ì¬')
        negative_count = sum(1 for a in analyzed if a['sentiment'] == 'ì•…ì¬')
        neutral_count = sum(1 for a in analyzed if a['sentiment'] == 'ì¤‘ë¦½')

        print("\n" + "=" * 80)
        print("ë¶„ì„ ì™„ë£Œ!")
        print("=" * 80)
        print(f"ì „ì²´ ê¸°ì‚¬: {len(analyzed)}ê±´")
        print(f"í˜¸ì¬ì„± ê¸°ì‚¬: {positive_count}ê±´")
        print(f"ì•…ì¬ì„± ê¸°ì‚¬: {negative_count}ê±´")
        print(f"ì¤‘ë¦½ ê¸°ì‚¬: {neutral_count}ê±´")
        print("=" * 80)

    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
