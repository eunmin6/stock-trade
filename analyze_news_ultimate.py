import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import os
import sys
import re
import feedparser
from newsapi import NewsApiClient

# UTF-8 ì¶œë ¥ ì„¤ì • (Windows)
if sys.platform == 'win32':
    os.system('chcp 65001 > nul')

# News API ì„¤ì • (ë¬´ë£Œ í”Œëœ: ì›” 500ê±´)
# API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” config íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¤ì„¸ìš”
# ë¬´ë£Œ ê°€ì…: https://newsapi.org/
NEWS_API_KEY = os.environ.get('NEWS_API_KEY', 'YOUR_API_KEY_HERE')  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°

def detect_category(keyword):
    """í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ë¥¼ ìë™ ì¸ì‹í•©ë‹ˆë‹¤."""

    # ê±°ì‹œê²½ì œ í‚¤ì›Œë“œ
    macro_keywords = [
        'ì¦ì‹œ', 'í™˜ìœ¨', 'ê¸ˆë¦¬', 'ìœ ê°€', 'ë‹¬ëŸ¬', 'ì›í™”', 'ì¸í”Œë ˆì´ì…˜',
        'ì—°ì¤€', 'Fed', 'ê¸°ì¤€ê¸ˆë¦¬', 'í†µí™”ì •ì±…', 'ê²½ì œ', 'GDP', 'CPI',
        'ë¬´ì—­', 'ê´€ì„¸', 'ê²½ê¸°', 'ì¹¨ì²´', 'ê²½ì œì„±ì¥', 'ì¬ì •', 'ì˜ˆì‚°'
    ]

    # ë¯¸êµ­ ì‹œì¥ í‚¤ì›Œë“œ
    us_market_keywords = [
        'ë¯¸êµ­', 'US', 'ë‚˜ìŠ¤ë‹¥', 'ë‹¤ìš°', 'S&P', 'NASDAQ', 'NYSE',
        'ì›”ìŠ¤íŠ¸ë¦¬íŠ¸', 'ì‹¤ë¦¬ì½˜ë°¸ë¦¬'
    ]

    # ì‚°ì—…/í…Œë§ˆ í‚¤ì›Œë“œ
    theme_keywords = [
        'AI', 'ë°˜ë„ì²´', '2ì°¨ì „ì§€', 'ë°”ì´ì˜¤', 'ì›ìë ¥', 'ìˆ˜ì†Œ', 'ESS',
        'ì „ê¸°ì°¨', 'EV', 'ë¡œë´‡', 'ìš°ì£¼', 'ë°©ì‚°', 'ì œì•½', 'IT',
        'í´ë¼ìš°ë“œ', 'ë©”íƒ€ë²„ìŠ¤', 'NFT', 'ë¸”ë¡ì²´ì¸'
    ]

    # ë¯¸êµ­ ê¸°ì—… í‚¤ì›Œë“œ
    us_companies = [
        'ì—”ë¹„ë””ì•„', 'NVIDIA', 'í…ŒìŠ¬ë¼', 'Tesla', 'ì• í”Œ', 'Apple',
        'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'Microsoft', 'ì•„ë§ˆì¡´', 'Amazon', 'êµ¬ê¸€', 'Google',
        'ë©”íƒ€', 'Meta', 'AMD', 'ì¸í…”', 'Intel'
    ]

    keyword_lower = keyword.lower()

    # ì¹´í…Œê³ ë¦¬ íŒì •
    if any(k in keyword for k in us_market_keywords):
        return 'ë¯¸êµ­ì¦ì‹œ'
    elif any(k in keyword for k in macro_keywords):
        return 'ê±°ì‹œê²½ì œ'
    elif any(k in keyword for k in us_companies):
        return 'ë¯¸êµ­ê¸°ì—…'
    elif any(k in keyword for k in theme_keywords):
        return 'ì‚°ì—…í…Œë§ˆ'
    else:
        return 'êµ­ë‚´ì¢…ëª©'

def translate_keyword_for_english(keyword):
    """í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ ê²€ìƒ‰ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    translation_map = {
        'ì—”ë¹„ë””ì•„': 'NVIDIA',
        'í…ŒìŠ¬ë¼': 'Tesla',
        'ì• í”Œ': 'Apple',
        'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸': 'Microsoft',
        'ì•„ë§ˆì¡´': 'Amazon',
        'êµ¬ê¸€': 'Google',
        'ë©”íƒ€': 'Meta',
        'ë¯¸êµ­ ì¦ì‹œ': 'US stock market',
        'ë‚˜ìŠ¤ë‹¥': 'NASDAQ',
        'ë‹¤ìš°': 'Dow Jones',
        'ë¯¸êµ­ ì¦ì‹œ í•«í•œ ì¢…ëª©': 'US hot stocks trending',
    }

    # ì§ì ‘ ë§¤ì¹­
    if keyword in translation_map:
        return translation_map[keyword]

    # ë¶€ë¶„ ë§¤ì¹­
    for kor, eng in translation_map.items():
        if kor in keyword:
            return eng

    # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    return keyword

def get_enhanced_keywords(category):
    """ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""

    # ê¸°ë³¸ í˜¸ì¬/ì•…ì¬ í‚¤ì›Œë“œ
    base_positive = [
        'ìƒìŠ¹', 'í˜¸ì¬', 'ì¦ê°€', 'ì„±ì¥', 'í™•ëŒ€', 'ê°œì„ ', 'í‘ì', 'ìˆ˜ì£¼', 'ê³„ì•½',
        'ì‹ ê·œ', 'ì¶œì‹œ', 'ê°œë°œ', 'ì„±ê³µ', 'í˜¸ì¡°', 'ìƒí–¥', 'ê¸ì •', 'íˆ¬ì', 'í˜‘ë ¥',
        'ì œíœ´', 'íŠ¹í—ˆ', 'ìˆ˜ì¶œ', 'ì‹¤ì ', 'ë°°ë‹¹', 'ì¦ì„¤', 'í™•ì¥', 'ì§„ì¶œ', 'ë„ì•½',
        'ìˆ˜ìµ', 'ë§¤ì¶œ', 'ìˆœì´ìµ', 'ëŒíŒŒ', 'ì‹ ê¸°ë¡', 'ìµœê³ ', 'ê¸‰ë“±', 'ê°•ì„¸',
        # ì˜ë¬¸ í‚¤ì›Œë“œ ì¶”ê°€
        'surge', 'soar', 'rally', 'gain', 'rise', 'jump', 'climb', 'advance',
        'bull', 'bullish', 'growth', 'profit', 'revenue', 'beat', 'outperform',
        'upgrade', 'positive', 'strong', 'boost', 'record', 'high', 'peak'
    ]

    base_negative = [
        'í•˜ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ë¶€ì§„', 'ì ì', 'ì·¨ì†Œ', 'ì§€ì—°', 'ì‹¤íŒ¨',
        'í•˜í–¥', 'ë¶€ì •', 'ë¦¬ìŠ¤í¬', 'ìš°ë ¤', 'ì†ì‹¤', 'ê°ì‚¬', 'ê·œì œ', 'ì†Œì†¡',
        'ì‚¬ê³ ', 'ë¬¸ì œ', 'ìœ„ë°˜', 'ì¤‘ë‹¨', 'íê¸°', 'ì² ìˆ˜', 'íŒŒì‚°', 'êµ¬ì¡°ì¡°ì •',
        'ì ì', 'ì†í•´', 'ê¸‰ë½', 'ì•½ì„¸', 'ìœ„ê¸°', 'ë¶ˆì•ˆ', 'ì¹¨ì²´',
        # ì˜ë¬¸ í‚¤ì›Œë“œ ì¶”ê°€
        'fall', 'drop', 'plunge', 'decline', 'slump', 'tumble', 'sink', 'slip',
        'bear', 'bearish', 'loss', 'miss', 'disappoint', 'downgrade', 'negative',
        'weak', 'concern', 'worry', 'risk', 'cut', 'low', 'crisis'
    ]

    # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ê°€ í‚¤ì›Œë“œ
    if category == 'ë¯¸êµ­ì¦ì‹œ':
        positive_add = [
            'ë ë¦¬', 'ì‚¬ìƒìµœê³ ', 'ATH', 'ì‚¬ê³ ê°€', 'ê¸°ë¡ê²½ì‹ ', 'ë¶ˆê¸°ë‘¥',
            'í˜¸í™©', 'ë°˜ë“±', 'ìƒìŠ¹ì¥', 'ê°•ì„¸ì¥', 'ì‹ ê³ ê°€', 'ê¸‰ë“±',
            'ë§¤ìˆ˜ì„¸', 'ìê¸ˆìœ ì…', 'ìˆ˜ê¸‰', 'ì¸ìƒ', 'í€ë”ë©˜í„¸', 'ë°¸ë¥˜ì—ì´ì…˜',
            'all-time high', 'breakout', 'momentum', 'strength'
        ]
        negative_add = [
            'í­ë½', 'ê¸‰ë½', 'ì•½ì„¸ì¥', 'í•˜ë½ì¥', 'ì¡°ì •', 'íŒ¨ë‹‰',
            'ì°¨ìµì‹¤í˜„', 'ë§¤ë„ì„¸', 'ìê¸ˆì´íƒˆ', 'ë²„ë¸”', 'ê³µí¬', 'ë³€ë™ì„±',
            'í•˜ë½ì„¸', 'ì¹¨ì²´', 'ìš°ë ¤', 'ê¸´ì¶•', 'ê¸ˆë¦¬ì¸ìƒ',
            'selloff', 'correction', 'volatility', 'crash', 'panic'
        ]

    elif category == 'ê±°ì‹œê²½ì œ':
        positive_add = [
            'ê¸ˆë¦¬ì¸í•˜', 'ì™„í™”', 'ë¶€ì–‘', 'ê²½ê¸°íšŒë³µ', 'í˜¸ì „', 'ë°˜ë“±',
            'í™•ì¥', 'ê°œì„ ', 'ì„±ì¥ë¥ ', 'ê³ ìš©ì¦ê°€', 'ì†Œë¹„ì¦ê°€', 'ìˆ˜ì¶œì¦ê°€',
            'íˆ¬ìì¦ê°€', 'ê²½ê¸°í™•ì¥', 'ì¸í•˜', 'ê²½ê¸°íšŒë³µ', 'ì•ˆì •',
            'rate cut', 'easing', 'stimulus', 'recovery', 'expansion'
        ]
        negative_add = [
            'ê¸ˆë¦¬ì¸ìƒ', 'ê¸´ì¶•', 'ê²½ê¸°ì¹¨ì²´', 'ë¶ˆí™©', 'ìœ„ì¶•', 'ë‘”í™”',
            'í•˜ë½', 'ê°ì†Œ', 'ë¦¬ì„¸ì…˜', 'ê²½ê¸°í›„í‡´', 'ê²½ê¸°ìœ„ì¶•', 'ì¸ìƒ',
            'ìœ„ê¸°', 'ë¶•ê´´', 'ë¶ˆì•ˆ', 'ê¸‰ë“±', 'ë¬¼ê°€ìƒìŠ¹',
            'rate hike', 'tightening', 'recession', 'slowdown', 'contraction'
        ]

    elif category in ['ë¯¸êµ­ê¸°ì—…', 'ì‚°ì—…í…Œë§ˆ', 'êµ­ë‚´ì¢…ëª©']:
        positive_add = [
            'FDAìŠ¹ì¸', 'ê³„ì•½ì²´ê²°', 'ìˆ˜ì£¼', 'ë§¤ì¶œì¦ê°€', 'ì‹ ì œí’ˆ', 'í˜ì‹ ',
            'ê¸°ìˆ ê°œë°œ', 'íŠ¹í—ˆë“±ë¡', 'ì¸ìˆ˜í•©ë³‘', 'M&A', 'íˆ¬ììœ ì¹˜', 'ìƒì¥',
            'IPO', 'ì‹œì¥ì ìœ ìœ¨', 'ê²½ìŸë ¥', 'ê¸€ë¡œë²Œì§„ì¶œ', 'íŒŒíŠ¸ë„ˆì‹­',
            'FDA approval', 'contract', 'deal', 'innovation', 'breakthrough',
            'partnership', 'collaboration', 'acquisition', 'expansion'
        ]
        negative_add = [
            'ë¦¬ì½œ', 'ì œì¬', 'ë²Œê¸ˆ', 'ì˜ì—…ì •ì§€', 'í—ˆê°€ì·¨ì†Œ', 'ì„ìƒì‹¤íŒ¨',
            'ê³„ì•½ì·¨ì†Œ', 'ìˆ˜ì£¼ì·¨ì†Œ', 'ì†ì‹¤', 'ë§¤ì¶œê°ì†Œ', 'ì‹œì¥ì´íƒˆ',
            'ê²½ìŸì‹¬í™”', 'ì ìœ ìœ¨í•˜ë½', 'ì‹¤ì ë¶€ì§„', 'ì ìì§€ì†',
            'recall', 'penalty', 'lawsuit', 'investigation', 'failure',
            'cancellation', 'delay', 'competition', 'challenge'
        ]
    else:
        positive_add = []
        negative_add = []

    return {
        'positive': base_positive + positive_add,
        'negative': base_negative + negative_add
    }

def get_report_path(keyword, date_str=None):
    """ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')

    # news í´ë” ìƒì„±
    report_dir = os.path.join('report', date_str, 'news')
    os.makedirs(report_dir, exist_ok=True)

    # íŒŒì¼ëª…ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
    safe_keyword = re.sub(r'[\\/*?:"<>|]', '', keyword)
    return os.path.join(report_dir, f'news_{safe_keyword}.md')

def crawl_google_news_rss(keyword, days=14):
    """Google News RSSë¥¼ í†µí•´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    print(f"\n[Google News RSS] '{keyword}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    articles = []

    try:
        # Google News RSS URL
        # í•œê¸€ í‚¤ì›Œë“œëŠ” URL ì¸ì½”ë”©
        import urllib.parse
        encoded_keyword = urllib.parse.quote(keyword)
        rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"

        # RSS íŒŒì‹±
        feed = feedparser.parse(rss_url)

        print(f"  ë°œê²¬ëœ RSS í•­ëª©: {len(feed.entries)}ê°œ")

        for entry in feed.entries[:30]:  # ìµœëŒ€ 30ê°œ
            try:
                title = entry.title if hasattr(entry, 'title') else ''
                link = entry.link if hasattr(entry, 'link') else ''
                content = entry.summary if hasattr(entry, 'summary') else ''

                # HTML íƒœê·¸ ì œê±°
                content = BeautifulSoup(content, 'html.parser').get_text(strip=True)

                # ë‚ ì§œ íŒŒì‹±
                if hasattr(entry, 'published_parsed'):
                    date = datetime(*entry.published_parsed[:6])
                else:
                    date = datetime.now()

                # ì¶œì²˜ ì¶”ì¶œ
                source = entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else 'Google News'

                articles.append({
                    'title': title,
                    'content': content,
                    'source': source,
                    'date': date,
                    'date_text': date.strftime('%Y-%m-%d %H:%M'),
                    'link': link
                })

            except Exception as e:
                continue

        print(f"  ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ")

    except Exception as e:
        print(f"  Google News RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return articles

def crawl_newsapi(keyword, days=14):
    """News APIë¥¼ í†µí•´ ì˜ë¬¸ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    print(f"\n[News API] '{keyword}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    articles = []

    # API í‚¤ í™•ì¸
    if NEWS_API_KEY == 'YOUR_API_KEY_HERE':
        print("  [WARNING] News API key is not configured.")
        print("  Set environment variable NEWS_API_KEY or enter directly in code.")
        print("  Free signup: https://newsapi.org/")
        return articles

    try:
        # News API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        newsapi = NewsApiClient(api_key=NEWS_API_KEY)

        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        to_date = datetime.now()
        from_date = to_date - timedelta(days=days)

        # ë‰´ìŠ¤ ê²€ìƒ‰
        all_articles = newsapi.get_everything(
            q=keyword,
            from_param=from_date.strftime('%Y-%m-%d'),
            to=to_date.strftime('%Y-%m-%d'),
            language='en',
            sort_by='relevancy',
            page_size=50  # ìµœëŒ€ 50ê°œ
        )

        print(f"  ë°œê²¬ëœ ê¸°ì‚¬: {len(all_articles.get('articles', []))}ê°œ")

        for article in all_articles.get('articles', []):
            try:
                title = article.get('title', '')
                content = article.get('description', '') or article.get('content', '')
                source = article.get('source', {}).get('name', 'Unknown')
                link = article.get('url', '')

                # ë‚ ì§œ íŒŒì‹±
                published_at = article.get('publishedAt', '')
                if published_at:
                    date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                else:
                    date = datetime.now()

                articles.append({
                    'title': title,
                    'content': content,
                    'source': source,
                    'date': date,
                    'date_text': date.strftime('%Y-%m-%d %H:%M'),
                    'link': link
                })

            except Exception as e:
                continue

        print(f"  ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ")

    except Exception as e:
        print(f"  News API ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        if "apiKey" in str(e):
            print("  [WARNING] API key is invalid. Check at https://newsapi.org/")

    return articles

def crawl_naver_news(keyword, days=14):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    print(f"\n[Naver News] '{keyword}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    # ë‚ ì§œ ê³„ì‚°
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)

    # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
    base_url = "https://search.naver.com/search.naver"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    articles = []

    # ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘ (ìµœëŒ€ 3í˜ì´ì§€)
    for page in range(1, 4):
        params = {
            'where': 'news',
            'query': keyword,
            'start': (page - 1) * 10 + 1,
            'sort': '1'  # ìµœì‹ ìˆœ ì •ë ¬
        }

        try:
            response = requests.get(base_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰
            news_items = soup.select('.vs1RfKE1eTzMZ5RqnhIv')

            if not news_items:
                break

            for item in news_items:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.select_one('a[data-heatmap-target=".tit"] span')
                    if not title_elem:
                        continue

                    # <mark> íƒœê·¸ ì œê±°
                    for mark in title_elem.find_all('mark'):
                        mark.unwrap()

                    title = title_elem.get_text(strip=True)

                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.select_one('a[data-heatmap-target=".tit"]')
                    link = link_elem.get('href', '') if link_elem else ''

                    # ë³¸ë¬¸ ìš”ì•½ ì¶”ì¶œ
                    content_elem = item.select_one('a[data-heatmap-target=".body"] span')
                    if content_elem:
                        for mark in content_elem.find_all('mark'):
                            mark.unwrap()
                        content = content_elem.get_text(strip=True)
                    else:
                        content = ''

                    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                    source_elem = item.select_one('a[data-heatmap-target=".prof"] span')
                    source = source_elem.get_text(strip=True) if source_elem else 'ì¶œì²˜ ë¯¸ìƒ'

                    # ë‚ ì§œ ì¶”ì¶œ
                    date_elem = item.select_one('.U1zN1wdZWj0pyvj9oyR0 span')
                    date_text = date_elem.get_text(strip=True) if date_elem else ''

                    # ë‚ ì§œ íŒŒì‹±
                    article_date = parse_date(date_text)

                    # ê¸°ê°„ í•„í„°ë§
                    if article_date and article_date < start_date:
                        continue

                    articles.append({
                        'title': title,
                        'content': content,
                        'source': source,
                        'date': article_date,
                        'date_text': date_text,
                        'link': link
                    })

                except Exception as e:
                    continue

            time.sleep(1)

        except requests.RequestException as e:
            break

    print(f"  ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ")

    return articles

def parse_date(date_text):
    """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        now = datetime.now()

        # "në¶„ ì „", "nì‹œê°„ ì „" í˜•ì‹
        if 'ë¶„ ì „' in date_text or 'ë¶„ì „' in date_text:
            minutes = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(minutes=minutes)
        elif 'ì‹œê°„ ì „' in date_text or 'ì‹œê°„ì „' in date_text:
            hours = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(hours=hours)
        elif 'ì¼ ì „' in date_text or 'ì¼ì „' in date_text:
            days = int(re.search(r'(\d+)', date_text).group(1))
            return now - timedelta(days=days)

        # "YYYY.MM.DD" í˜•ì‹
        date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_text)
        if date_match:
            year, month, day = date_match.groups()
            return datetime(int(year), int(month), int(day))

        return None

    except:
        return None

def crawl_ultimate_news(keyword, days=14):
    """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤: Google RSS + News API + Naver"""

    # ì¹´í…Œê³ ë¦¬ ê°ì§€
    category = detect_category(keyword)

    all_articles = []

    # 1. Google News RSS (í•œê¸€ + ì˜ë¬¸ ëª¨ë‘ ì§€ì›)
    google_articles = crawl_google_news_rss(keyword, days)
    all_articles.extend(google_articles)

    # 2. News API (ì˜ë¬¸ ë‰´ìŠ¤ - ë¯¸êµ­ ì¦ì‹œ/ê¸°ì—…ì¸ ê²½ìš°)
    if category in ['ë¯¸êµ­ì¦ì‹œ', 'ë¯¸êµ­ê¸°ì—…']:
        english_keyword = translate_keyword_for_english(keyword)
        newsapi_articles = crawl_newsapi(english_keyword, days)
        all_articles.extend(newsapi_articles)

    # 3. ë„¤ì´ë²„ ë‰´ìŠ¤ (í•œê¸€ ë‰´ìŠ¤)
    naver_articles = crawl_naver_news(keyword, days)
    all_articles.extend(naver_articles)

    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    seen_titles = set()
    unique_articles = []

    for article in all_articles:
        # ì œëª© ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        normalized_title = re.sub(r'[\s\W]+', '', article['title'].lower())

        if normalized_title not in seen_titles and normalized_title:
            seen_titles.add(normalized_title)
            unique_articles.append(article)

    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    unique_articles.sort(key=lambda x: x['date'] if x['date'] else datetime.min, reverse=True)

    print(f"\n{'='*80}")
    print(f"ì´ {len(unique_articles)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    print(f"  - Google News RSS: {len(google_articles)}ê°œ")
    print(f"  - News API: {len([a for a in all_articles if a['source'] not in ['Google News', 'ì¶œì²˜ ë¯¸ìƒ'] and a not in google_articles and a not in naver_articles])}ê°œ")
    print(f"  - Naver News: {len(naver_articles)}ê°œ")
    print(f"  - ì¤‘ë³µ ì œê±° í›„: {len(unique_articles)}ê°œ")
    print(f"{'='*80}\n")

    return unique_articles

def analyze_sentiment(articles, category):
    """ê¸°ì‚¬ì˜ í˜¸ì¬/ì•…ì¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ í‚¤ì›Œë“œ ê¸°ë°˜)"""

    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    keywords = get_enhanced_keywords(category)
    positive_keywords = keywords['positive']
    negative_keywords = keywords['negative']

    analyzed = []

    for article in articles:
        text = (article['title'] + ' ' + article['content']).lower()

        # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
        positive_count = sum(1 for keyword in positive_keywords if keyword.lower() in text)
        negative_count = sum(1 for keyword in negative_keywords if keyword.lower() in text)

        # ê°ì„± ë¶„ë¥˜
        if positive_count > negative_count:
            sentiment = 'í˜¸ì¬'
            score = min(positive_count, 5)  # ìµœëŒ€ 5ì 
        elif negative_count > positive_count:
            sentiment = 'ì•…ì¬'
            score = min(negative_count, 5)
        else:
            sentiment = 'ì¤‘ë¦½'
            score = 0

        # ì¤‘ìš”ë„ ê³„ì‚° (í˜¸ì¬/ì•…ì¬ í‚¤ì›Œë“œ ê°œìˆ˜ + ìµœì‹ ì„±)
        importance = positive_count + negative_count

        analyzed.append({
            **article,
            'sentiment': sentiment,
            'score': score,
            'importance': importance,
            'positive_count': positive_count,
            'negative_count': negative_count
        })

    return analyzed

def generate_enhanced_report(keyword, analyzed_articles, category, date_str=None):
    """ê°œì„ ëœ ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""

    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')

    # í˜¸ì¬/ì•…ì¬/ì¤‘ë¦½ ë¶„ë¥˜ ë° ì¤‘ìš”ë„ìˆœ ì •ë ¬
    positive_news = sorted([a for a in analyzed_articles if a['sentiment'] == 'í˜¸ì¬'],
                          key=lambda x: x['importance'], reverse=True)
    negative_news = sorted([a for a in analyzed_articles if a['sentiment'] == 'ì•…ì¬'],
                          key=lambda x: x['importance'], reverse=True)
    neutral_news = [a for a in analyzed_articles if a['sentiment'] == 'ì¤‘ë¦½']

    # ì¹´í…Œê³ ë¦¬ë³„ ì•„ì´ì½˜
    category_icons = {
        'ë¯¸êµ­ì¦ì‹œ': 'ğŸ‡ºğŸ‡¸',
        'ê±°ì‹œê²½ì œ': 'ğŸ“Š',
        'ë¯¸êµ­ê¸°ì—…': 'ğŸ¢',
        'ì‚°ì—…í…Œë§ˆ': 'ğŸ­',
        'êµ­ë‚´ì¢…ëª©': 'ğŸ“ˆ'
    }

    icon = category_icons.get(category, 'ğŸ“°')

    md_content = f"""# {icon} {keyword} ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ (Ultimate Multi-Source)

**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ**: {date_str}
**ìˆ˜ì§‘ ê¸°ê°„**: ìµœê·¼ 14ì¼
**ì¹´í…Œê³ ë¦¬**: {category}
**ë°ì´í„° ì†ŒìŠ¤**: Google News RSS, News API, Naver News

---

## ğŸ“Š ìš”ì•½ í†µê³„

| êµ¬ë¶„ | ê±´ìˆ˜ | ë¹„ìœ¨ |
|------|------|------|
| **ì „ì²´ ê¸°ì‚¬** | {len(analyzed_articles)}ê±´ | 100% |
| ğŸŸ¢ **í˜¸ì¬ì„± ê¸°ì‚¬** | {len(positive_news)}ê±´ | {len(positive_news)/len(analyzed_articles)*100:.1f}% |
| ğŸ”´ **ì•…ì¬ì„± ê¸°ì‚¬** | {len(negative_news)}ê±´ | {len(negative_news)/len(analyzed_articles)*100:.1f}% |
| âšª **ì¤‘ë¦½ ê¸°ì‚¬** | {len(neutral_news)}ê±´ | {len(neutral_news)/len(analyzed_articles)*100:.1f}% |

### ğŸ’¹ ì‹œì¥ ë¶„ìœ„ê¸°
"""

    # ì‹œì¥ ë¶„ìœ„ê¸° íŒë‹¨
    sentiment_ratio = len(positive_news) / len(analyzed_articles) if analyzed_articles else 0

    if sentiment_ratio >= 0.7:
        market_mood = "**ë§¤ìš° ê¸ì •ì ** ğŸ”¥ - ê°•í•œ í˜¸ì¬ ë‰´ìŠ¤ ìš°ì„¸"
    elif sentiment_ratio >= 0.5:
        market_mood = "**ê¸ì •ì ** âœ… - í˜¸ì¬ ë‰´ìŠ¤ ìš°ì„¸"
    elif sentiment_ratio >= 0.4:
        market_mood = "**ì¤‘ë¦½ì ** âš–ï¸ - í˜¸ì¬ì™€ ì•…ì¬ ê· í˜•"
    elif sentiment_ratio >= 0.2:
        market_mood = "**ë¶€ì •ì ** âš ï¸ - ì•…ì¬ ë‰´ìŠ¤ ìš°ì„¸"
    else:
        market_mood = "**ë§¤ìš° ë¶€ì •ì ** ğŸš¨ - ê°•í•œ ì•…ì¬ ë‰´ìŠ¤ ìš°ì„¸"

    md_content += f"- {market_mood}\n"
    md_content += f"- í˜¸ì¬/ì•…ì¬ ë¹„ìœ¨: {len(positive_news)}:{len(negative_news)}\n\n"

    md_content += "---\n\n"

    # í˜¸ì¬ì„± ë‰´ìŠ¤
    md_content += f"## ğŸŸ¢ í˜¸ì¬ì„± ë‰´ìŠ¤ ({len(positive_news)}ê±´)\n\n"

    if positive_news:
        for idx, article in enumerate(positive_news, 1):
            md_content += f"""### {idx}. {article['title']}

- **ì¶œì²˜**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}
- **ì¤‘ìš”ë„**: {'â­' * article['score']} ({article['positive_count']}ê°œ í˜¸ì¬ í‚¤ì›Œë“œ)

**ğŸ“ ìš”ì•½**:
{article['content'] if article['content'] else '(ìš”ì•½ ì—†ìŒ)'}

[ğŸ“° ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "í˜¸ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    # ì•…ì¬ì„± ë‰´ìŠ¤
    md_content += f"## ğŸ”´ ì•…ì¬ì„± ë‰´ìŠ¤ ({len(negative_news)}ê±´)\n\n"

    if negative_news:
        for idx, article in enumerate(negative_news, 1):
            md_content += f"""### {idx}. {article['title']}

- **ì¶œì²˜**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}
- **ì¤‘ìš”ë„**: {'â­' * article['score']} ({article['negative_count']}ê°œ ì•…ì¬ í‚¤ì›Œë“œ)

**ğŸ“ ìš”ì•½**:
{article['content'] if article['content'] else '(ìš”ì•½ ì—†ìŒ)'}

[ğŸ“° ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "ì•…ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    # ì¤‘ë¦½ ë‰´ìŠ¤ (ìµœëŒ€ 10ê°œ)
    md_content += f"## âšª ì¤‘ë¦½ ë‰´ìŠ¤ ({len(neutral_news)}ê±´)\n\n"

    if neutral_news:
        display_count = min(len(neutral_news), 10)
        md_content += f"*ìµœì‹  {display_count}ê±´ë§Œ í‘œì‹œ*\n\n"

        for idx, article in enumerate(neutral_news[:display_count], 1):
            md_content += f"""### {idx}. {article['title']}

- **ì¶œì²˜**: {article['source']}
- **ë‚ ì§œ**: {article['date_text']}

**ğŸ“ ìš”ì•½**:
{article['content'] if article['content'] else '(ìš”ì•½ ì—†ìŒ)'}

[ğŸ“° ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°]({article['link']})

---

"""
    else:
        md_content += "ì¤‘ë¦½ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n---\n\n"

    # ë¶„ì„ ì¸ì‚¬ì´íŠ¸
    md_content += """## ğŸ’¡ ë¶„ì„ ì¸ì‚¬ì´íŠ¸

### ğŸ“Œ ì£¼ìš” í¬ì¸íŠ¸
"""

    # Top 3 í˜¸ì¬/ì•…ì¬ ì¶”ì¶œ
    if positive_news:
        md_content += "\n#### ğŸŸ¢ ì£¼ìš” í˜¸ì¬\n"
        for i, article in enumerate(positive_news[:3], 1):
            md_content += f"{i}. {article['title']}\n"

    if negative_news:
        md_content += "\n#### ğŸ”´ ì£¼ìš” ì•…ì¬\n"
        for i, article in enumerate(negative_news[:3], 1):
            md_content += f"{i}. {article['title']}\n"

    md_content += """

### âš ï¸ íˆ¬ì ì‹œ ìœ ì˜ì‚¬í•­
- ë³¸ ë¶„ì„ì€ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ë¶„ì„ìœ¼ë¡œ **ì°¸ê³ ìš©**ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì‹¤ì œ íˆ¬ì ê²°ì •ì€ **ì „ë¬¸ê°€ì˜ ì¡°ì–¸**ê³¼ **ì¢…í•©ì ì¸ ë¶„ì„**ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ë‰´ìŠ¤ì˜ ì¤‘ìš”ë„ëŠ” í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¨ìˆœ ì§€í‘œì…ë‹ˆë‹¤.
- ìµœì‹  ë‰´ìŠ¤ì¼ìˆ˜ë¡ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“Œ ì°¸ê³ ì‚¬í•­

- **ë¶„ì„ ë°©ë²•**: Ultimate Multi-Source í¬ë¡¤ë§ + í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„
- **ë°ì´í„° ì†ŒìŠ¤**:
  - Google News RSS (í•œê¸€/ì˜ë¬¸ í†µí•©)
  - News API (ì˜ë¬¸ ì „ë¬¸ ë‰´ìŠ¤, ì›” 500ê±´ ë¬´ë£Œ)
  - Naver News (í•œê¸€ ë‰´ìŠ¤)
- **ë¶„ì„ í‚¤ì›Œë“œ ìˆ˜**: í˜¸ì¬/ì•…ì¬ í‚¤ì›Œë“œ ê°ê° ì•½ 70ê°œ (ì˜ë¬¸ í¬í•¨)
- **ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ ë¶„ì„**: ë¯¸êµ­ì¦ì‹œ, ê±°ì‹œê²½ì œ, ì‚°ì—…í…Œë§ˆ ë“± ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì ìš©
- **ì¤‘ë³µ ì œê±°**: ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±°ë¡œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ë§Œ ì„ ë³„

---

*ğŸ¤– Generated with Claude Code (Ultimate Multi-Source Version)*
*ğŸ“… Generated at: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + "*"

    output_file = get_report_path(keyword, date_str)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"\n[OK] ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return output_file

def safe_print(text):
    """ì´ëª¨ì§€ë¥¼ ì•ˆì „í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    try:
        print(text)
    except UnicodeEncodeError:
        # ì´ëª¨ì§€ ì œê±° í›„ ì¶œë ¥
        text_clean = text.encode('ascii', 'ignore').decode('ascii')
        print(text_clean)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    safe_print("ğŸ“° Ultimate Multi-Source ë‰´ìŠ¤ ë¶„ì„ ë„êµ¬")
    safe_print("   (Google News RSS + News API + Naver News)")
    print("=" * 80)

    # News API í‚¤ í™•ì¸
    if NEWS_API_KEY == 'YOUR_API_KEY_HERE':
        print("\n[WARNING] News API key is not configured!")
        print("   Using only Google RSS + Naver (News API skipped)")
        print("   Free signup: https://newsapi.org/")
        print("   Set environment variable: set NEWS_API_KEY=your_api_key")
        print()

    # í‚¤ì›Œë“œ ì…ë ¥
    if len(sys.argv) > 1:
        keyword = sys.argv[1]
    else:
        keyword = input("\në¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ëª©ëª…/í…Œë§ˆ/ì‹œì¥): ").strip()

    if not keyword:
        print("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    try:
        # ì¹´í…Œê³ ë¦¬ ìë™ ì¸ì‹
        category = detect_category(keyword)
        safe_print(f"\nğŸ·ï¸  ì¸ì‹ëœ ì¹´í…Œê³ ë¦¬: {category}")

        # 1. Ultimate ë©€í‹° ì†ŒìŠ¤ ë‰´ìŠ¤ í¬ë¡¤ë§
        articles = crawl_ultimate_news(keyword, days=14)

        if not articles:
            print("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. ê°ì„± ë¶„ì„ (ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤)
        safe_print(f"\nğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì¤‘... (ì¹´í…Œê³ ë¦¬: {category})")
        analyzed = analyze_sentiment(articles, category)

        # 3. ê°œì„ ëœ ë¦¬í¬íŠ¸ ìƒì„±
        output_file = generate_enhanced_report(keyword, analyzed, category)

        # 4. ìš”ì•½ ì¶œë ¥
        positive_count = sum(1 for a in analyzed if a['sentiment'] == 'í˜¸ì¬')
        negative_count = sum(1 for a in analyzed if a['sentiment'] == 'ì•…ì¬')
        neutral_count = sum(1 for a in analyzed if a['sentiment'] == 'ì¤‘ë¦½')

        print("\n" + "=" * 80)
        safe_print("âœ… ë¶„ì„ ì™„ë£Œ!")
        print("=" * 80)
        safe_print(f"ğŸ“° ì „ì²´ ê¸°ì‚¬: {len(analyzed)}ê±´")
        safe_print(f"ğŸŸ¢ í˜¸ì¬ì„± ê¸°ì‚¬: {positive_count}ê±´ ({positive_count/len(analyzed)*100:.1f}%)")
        safe_print(f"ğŸ”´ ì•…ì¬ì„± ê¸°ì‚¬: {negative_count}ê±´ ({negative_count/len(analyzed)*100:.1f}%)")
        safe_print(f"âšª ì¤‘ë¦½ ê¸°ì‚¬: {neutral_count}ê±´ ({neutral_count/len(analyzed)*100:.1f}%)")
        safe_print(f"\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬: {category}")
        safe_print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
        print("=" * 80)

    except Exception as e:
        safe_print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
